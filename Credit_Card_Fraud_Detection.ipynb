{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Credit_Card_Fraud_Detection.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"v1rzeZX5ED3k"},"source":["# **Credit Card Fraud Detection**"]},{"cell_type":"markdown","metadata":{"id":"FOKutHbwEKeU"},"source":["**Problem**\n","\n","In recent times, many people moved to digital life for purchasing, and people prefer using credit/ debit cards for making transactions. Cyber attackers are misusing features like credit limits. As technology advances, new methods of cyber-attacks are emerging. This has become a significant problem in the modern era, as all transactions can be quickly completed online by only entering your credit card information.\n","\n","**Approach using machine Learning:**   \n","\n","\n","When we talk about security in digital life, the main challenge is to find fraudulent activity. Fraud detection is a set of actions taken to prevent money from being accessed illegally. In this project, we use three machine learning algorithms to perform classifications of abnormal activities. Algorithms track the patterns of transactions, and if they find anything fraudulent in the transaction, it should abort the transaction. We use Accuracy, Precision, Recall, and F1 score as deciding factors to choose which algorithm is the best fit for this data."]},{"cell_type":"markdown","metadata":{"id":"8Wd2Efs2EXSB"},"source":["**Dataset:**\n","\n","We are using a dataset from [kaggle] (https://www.kaggle.com/mlg-ulb/creditcardfraud . Please download this dataset or the csv file on the shared folder (preferred) and upload to the runtime session. \n","\n","This dataset contains transactions made by credit cards in September 2013 by European cardholders. We have 30 features and one final class column. \n","\n","Here dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. \n","\n","The dataset is highly unbalanced; it has positive class (frauds) account for 0.172% of all transactions.\n","It contains only numerical input variables, which are the result of a PCA transformation."]},{"cell_type":"markdown","metadata":{"id":"TDnoEQTWStTz"},"source":["**Importing the Dependencies**"]},{"cell_type":"code","metadata":{"id":"hlNfrSC1PGfZ"},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from matplotlib import gridspec\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import precision_score, recall_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import confusion_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DqfXePq1zVUO","outputId":"5a95d304-803c-4857-f0e0-669216e1620e"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"WBSFbAyZ6Fx8"},"source":["The following link is one of a shared folder in which our csv file is uploaded\n","Please download this file and upload it to the colab session to run it\n","https://drive.google.com/drive/folders/1myGUb3fehpbl-ohiWDxNtKfcYfFe0UYi?usp=sharing"]},{"cell_type":"code","metadata":{"id":"owQxLK4qycby"},"source":["# loading the dataset to a Pandas DataFrame\n","\n","path=\"/content/drive/MyDrive/credit card /creditcard.csv\"\n","ccData = pd.read_csv(path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LNQYLn7tZ2Ib"},"source":["## Exploratory Data Analysis"]},{"cell_type":"code","metadata":{"id":"dyvubGGMMd7G"},"source":["ccData.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o_iMfyHsTa6s"},"source":["# first 5 rows of the dataset\n","ccData.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iWCP6YJjThIM"},"source":["ccData.tail()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KcCZquvEUD3A"},"source":["# dataset informations\n","ccData.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XxwasmfKUQiA"},"source":["# checking the number of missing values in each column\n","ccData.isnull().sum()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OJn79SFyYVUK"},"source":["ccData['Class'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4OlMhkHzVKMv"},"source":["0 --> Normal Transaction\n","\n","1 --> fraudulent transaction"]},{"cell_type":"code","metadata":{"id":"aWYFXN_wWO_-"},"source":["ccData.drop_duplicates(inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VCEl9pC1Ee-C"},"source":["**Removing Duplicate Transactions**\n","\n","Duplicates are a type of nonrandom sampling that can cause your fitted model to be biased. By including them, the model will basically overfit this subset of points. So, we will remove all duplicate transactions. "]},{"cell_type":"code","metadata":{"id":"EIpoKfp5Ugri"},"source":["# distribution of legit transactions & fraudulent transactions\n","ccData['Class'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kpEoxG2vFPV6"},"source":["The credit card dataset had 284807 transactions initially, after removing the duplicates the data set has 283726 transactions."]},{"cell_type":"markdown","metadata":{"id":"flz_InK7VGri"},"source":["This dataset is highly imblanced (as shown below)"]},{"cell_type":"code","metadata":{"id":"2D1MPf7fg-Q7"},"source":["labels = [\"Normal\", \"Fraud\"]\n","plt.figure(figsize=(10, 10))\n","ccData['Class'].plot(kind = \"hist\",rot = 0)\n","plt.title(\"Class Imbalance\")\n","plt.ylabel(\"Count\")\n","plt.xticks(range(2),labels)\n","plt.show"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QNn77bdbU2Lf"},"source":["# separating the data for analysis\n","normal = ccData[ccData.Class == 0]\n","fraud = ccData[ccData.Class == 1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zsrMgOdoVnrV"},"source":["print(normal.shape)\n","print(fraud.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1s3KGshBVsTb"},"source":["# statistical measures of the data\n","normal.Amount.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KkU3Mzr5V7fR"},"source":["fraud.Amount.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aFUMMHwYWMvp"},"source":["# compare the values for both transactions\n","ccData.groupby('Class').mean()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ybHNYPpEW0N6"},"source":["## Under-Sampling"]},{"cell_type":"markdown","metadata":{"id":"MlXkIGEIW3KM"},"source":["Build a sample dataset containing similar distribution of normal transactions and fraudulent fransactions"]},{"cell_type":"markdown","metadata":{"id":"bCj3Dee7XB0F"},"source":["Number of fraudulent transactions = 473"]},{"cell_type":"code","metadata":{"id":"QtWT13mKWjJ_"},"source":["sampled_normal = normal.sample(n=473)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SNiYI_SmXeim"},"source":["**Concatenating two DataFrames**"]},{"cell_type":"code","metadata":{"id":"0yiXrYiRXcnE"},"source":["ccData_new = pd.concat([sampled_normal, fraud], axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UuLw43AXX0bq"},"source":["ccData_new.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n_Rjt1qDX3AQ"},"source":["ccData_new.tail()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bFsRcj0gX-3M"},"source":["ccData_new['Class'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aeIPmzkphTS_"},"source":["labels = [\"Normal\", \"Fraud\"]\n","plt.figure(figsize=(10, 10))\n","ccData_new['Class'].plot(kind = \"hist\",rot = 0)\n","plt.title(\"After under sampling\")\n","plt.ylabel(\"Count\")\n","plt.xticks(range(2),labels)\n","plt.show"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jan1xMeWYLrM"},"source":["ccData_new.groupby('Class').mean()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wbe0g12oYhyK"},"source":["## Splitting the data into Features & Targets"]},{"cell_type":"code","metadata":{"id":"oiC0OOyUYUoD"},"source":["X = ccData_new.drop(columns='Class', axis=1)\n","Y = ccData_new['Class']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9u7wbWqzYyrI"},"source":["print(X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EHnRPzZvYz-F"},"source":["print(Y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"78wEbV41Y6j3"},"source":["## Split the data into Training data & Testing Data"]},{"cell_type":"markdown","metadata":{"id":"sOemrxVKNmfu"},"source":["To model a general case, we have split our existing data into training and testing sets where 20% of the data is set to test and 80% of data is used to train the model. "]},{"cell_type":"code","metadata":{"id":"FrsqqwT0Y3n5"},"source":["X_train, X_test, Y_train, Y_test = train_test_split(X, Y,test_size=0.2, random_state=16)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FUmwXz99ZuvK"},"source":["print(X.shape, X_train.shape, X_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p99vuuP6FWze"},"source":["**Feature Scaling**"]},{"cell_type":"markdown","metadata":{"id":"HTxjKq8-Vr4c"},"source":["We first standardize the data since otherwise, a feature with high variation may bias or dictate the distance functions. Thus, we use StandardScaler that centers the data (by subtracting the mean of data) and make the variance 1 (by dividing by variance) to ensure that no single feature dominates its functioning. "]},{"cell_type":"code","metadata":{"id":"r3x0hloZQ0-7"},"source":["from sklearn.preprocessing import StandardScaler\n","sc = StandardScaler()\n","sc.fit(X_train)\n","\n","X_train = sc.transform(X_train)\n","X_test = sc.transform(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aowgjqbeZ7d1"},"source":["## Model Training"]},{"cell_type":"markdown","metadata":{"id":"i0NkjaxyZ96E"},"source":["**Logisitc Regression**"]},{"cell_type":"markdown","metadata":{"id":"fYaE6oKXFxY7"},"source":["Logistic regression is a widely used discriminative classification model. Since our data has a binary classification, we will use binary logistic regression. Binary Logistic Regression Classification uses one or more predictor variables that can be either continuous or categorical to predict the target variable classes. This technique aids in identifying essential features(Xi) that influence the target variable (Y) and the nature of the relationships between these features and the dependent variable."]},{"cell_type":"code","metadata":{"id":"i0HHd1v8Z2Mq"},"source":["model = LogisticRegression()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_rshSouvaGrv"},"source":["# training the Logistic Regression Model with Training Data\n","model.fit(X_train, Y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H9FYWyKjalvC"},"source":["Model Evaluation"]},{"cell_type":"code","metadata":{"id":"pC2kCJhOaage"},"source":["# accuracy on training data\n","train_pred = model.predict(X_train)\n","acc_train = accuracy_score(train_pred, Y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ijq6gAevbDwm"},"source":["print('Accuracy on Training data : ', acc_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tryJUs6mbJM7"},"source":["# accuracy on test data\n","Y_pred = model.predict(X_test)\n","acc = accuracy_score(Y_pred, Y_test)*100"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lxEuN24ib3hS"},"source":["print('Accuracy score on Test Data : ', acc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yp_Aau1RPP3-"},"source":["rec = recall_score(Y_pred, Y_test)*100\n","print('Recall score on Test Data : ', rec)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4eE6vHaYPP6l"},"source":["f1 = f1_score(Y_pred, Y_test)*100\n","print('F1 score on Test Data : ', f1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iwHshfEEPP9U"},"source":["prec = precision_score(Y_pred,Y_test)*100\n","print('Precision on Test Data : ', prec)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jtd6uYCfP_pp"},"source":["LABELS = ['Normal', 'Fraud']\n","conf_matrix = confusion_matrix(Y_test,Y_pred)\n","sns.heatmap(conf_matrix, xticklabels = LABELS, \n","            yticklabels = LABELS, annot = True, fmt =\"d\");\n","plt.title(\"Confusion matrix\")\n","plt.ylabel('True class')\n","plt.xlabel('Predicted class')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SLRIW81OSBkP"},"source":["Out of 90 normal transactions, the logistic regression model predicted 86 of them correctly and four wrong. Out of 100 fraudulent transactions, the logistic regression model predicted 93 of them correctly and seven bad. In conclusion, the logistic regression model classified 176 data points correctly out of 190."]},{"cell_type":"markdown","metadata":{"id":"_OPNKidqU5R_"},"source":["**Random Forest Classifier**"]},{"cell_type":"markdown","metadata":{"id":"V5ZCEN7lITHL"},"source":["A random forest is a supervised machine learning system that uses decision tree algorithms to build it. Many decision trees make up a random forest algorithm. Bagging or bootstrap aggregation are used to train the 'forest' formed by the random forest method. Bagging is a meta-algorithm that increases the accuracy of machine learning methods by grouping them. The algorithm determines the outcome based on decision tree predictions. It forecasts by averaging the output of various trees."]},{"cell_type":"code","metadata":{"id":"wvJYhde9VM5H"},"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","rf_model = RandomForestClassifier(max_depth = 4)\n","rf_model.fit(X_train, Y_train)\n","rf_pred = rf_model.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GVOt0h33Z6l8"},"source":["rf_acc = accuracy_score(rf_pred, Y_test)*100\n","print('Accuracy score on Test Data : ', rf_acc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GgpqyVQSc-gO"},"source":["rf_rec = recall_score(rf_pred, Y_test)*100\n","print('Recall score on Test Data : ', rf_rec)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DA0Hgm3qdP-K"},"source":["rf_prec = precision_score(rf_pred, Y_test)*100\n","print('Precision score on Test Data : ', rf_prec)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qB83iLCJddtO"},"source":["rf_f1 = f1_score(rf_pred, Y_test)*100\n","print('F1 score on Test Data : ', rf_f1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zLvXIou_Y57X"},"source":["rfCM = confusion_matrix(Y_test, rf_pred)\n","sns.heatmap(rfCM, xticklabels = LABELS, \n","            yticklabels = LABELS, annot = True, fmt =\"d\");\n","plt.title(\"Confusion matrix\")\n","plt.ylabel('True class')\n","plt.xlabel('Predicted class')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S_AzVR4padyi"},"source":["Out of 90 normal and 100 fraud transactions, the Random forest model correctly predicted 89 normal transactions and 91 fraud transactions. In conclusion, the Random forest model predicts 178 data points correctly out of 190."]},{"cell_type":"markdown","metadata":{"id":"CaWchdeHQrnn"},"source":["**K-nearest Neighbours Classifier**"]},{"cell_type":"markdown","metadata":{"id":"wIMZtinTXOXj"},"source":["k-NN is a supervised algorithm used to solve classification problems. In K-NN, the data point of interest (unseen, unlabelled data) is assigned a class based on the classes of the k closest known data points around it. The value of k determines how many neighboring points we consider to determine its class (based on the majority class value of k points nearest to the point). The main crux of this algorithm is to find the optimized values for the k parameter. Too large or too small of a value will produce incorrect results since the former may bias the data against classes with fewer samples while the latter will make the model susceptible to outliers."]},{"cell_type":"markdown","metadata":{"id":"3n-oJwxXV-y_"},"source":["In our case, we have plotted a graph of the mean error values encountered using different values of k (form 1 to 40). In our case, k=8 gives the least mean error (maximum accuracy), so we proceed with that."]},{"cell_type":"code","metadata":{"id":"etxddoiESXF8"},"source":["from sklearn.neighbors import KNeighborsClassifier\n","error = []\n","\n","# Calculating error for K values between 1 and 40\n","for i in range(1, 40):\n","    knn_model = KNeighborsClassifier(n_neighbors=i)\n","    knn_model.fit(X_train, Y_train)\n","    pred_i = knn_model.predict(X_test)\n","    error.append(np.mean(pred_i != Y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fq66g2PZSrPM"},"source":["plt.figure(figsize=(12, 6))\n","plt.plot(range(1, 40), error, marker='o', markersize=10)\n","plt.title('Error Rate K Value')\n","plt.xlabel('K Value')\n","plt.ylabel('Mean Error')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W77nzZcSS7bu"},"source":["from sklearn.neighbors import KNeighborsClassifier\n","classifier = KNeighborsClassifier(n_neighbors=7)\n","classifier.fit(X_train, Y_train)\n","knn_pred = classifier.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wXuKZpLGTkjj"},"source":["knn_acc = accuracy_score(knn_pred, Y_test)*100\n","print('Accuracy score on Test Data : ', knn_acc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EW165gG1cc7-"},"source":["knn_rec = recall_score(knn_pred, Y_test)*100\n","print('Recall score on Test Data : ', knn_rec)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"td2p6vMpcyBD"},"source":["knn_prec = precision_score(knn_pred, Y_test)*100\n","print('Precision score on Test Data : ', knn_prec)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mB8Fsr0Rc2wT"},"source":["knn_f1 = f1_score(knn_pred, Y_test)*100\n","print('F1 score on Test Data : ', knn_f1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YMnRS0NzTf3e"},"source":["knnCM = confusion_matrix(Y_test, knn_pred)\n","sns.heatmap(knnCM, xticklabels = LABELS, \n","            yticklabels = LABELS, annot = True, fmt =\"d\");\n","plt.title(\"Confusion matrix\")\n","plt.ylabel('True class')\n","plt.xlabel('Predicted class')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HarDVOGXbb61"},"source":["Out of 90 normal transactions and 100 fraud transactions, the KNN model predicted 87 of normal transactions and 92 of fraud transactions correctly. In conclusion, the KNN model classified 178 data points correctly out of 190."]},{"cell_type":"markdown","metadata":{"id":"p2Vc9FmxVTLK"},"source":["# Results"]},{"cell_type":"code","metadata":{"id":"jNQgeDC_az1b"},"source":["comp = {'Algorithm': ['Logistic Regression', 'K-Nearest Neighbours', 'Random Forest'], \n","        'Accuracy': [acc,knn_acc,rf_acc],\n","        'Recall': [rec, knn_rec, rf_rec],\n","        'Precision': [prec, knn_prec, rf_prec],\n","        'F1 Score': [f1,knn_f1,rf_f1]} "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c2-QH1JReu7y"},"source":["df = pd.DataFrame(comp)  \n","df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bEQNcSWSXaqS"},"source":["# Conclusion\n","In this project, we have tested three different algorithms, i.e., Logistic Regression,  K-nearest neighbors, and Random Forest, to predict the nature of a credit card transaction. After training and testing the three models, we observed that all performed equally well, but Random forest performed better in a few aspects."]}]}